{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:09<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight diff for down_blocks.1.attentions.0.norm.bias: 0.000795598840340972\n",
      "Weight diff for down_blocks.1.attentions.0.proj_in.bias: 0.00035599491093307734\n",
      "Weight diff for down_blocks.1.attentions.0.proj_in.weight: 0.00046758464304730296\n",
      "Weight diff for down_blocks.1.attentions.0.proj_out.bias: 0.0003796196833718568\n",
      "Weight diff for down_blocks.1.attentions.0.proj_out.weight: 0.0006582210771739483\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight: 0.0008608432835899293\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias: 0.00046892924001440406\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight: 0.0005575929535552859\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight: 0.0005012233159504831\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight: 0.0005378096248023212\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight: 0.000952545611653477\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias: 0.00046708661830052733\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight: 0.0008946539019234478\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight: 0.0006990322726778686\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight: 0.0007740414584986866\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias: 0.00032313683186657727\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight: 0.0010628688614815474\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias: 0.00035072871833108366\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight: 0.0007191819022409618\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias: 0.00037857983261346817\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias: 0.0008552848594263196\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias: 0.0005226641660556197\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight: 0.0006830492056906223\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias: 0.0003462405293248594\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight: 0.0005882489494979382\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight: 0.0005927770980633795\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight: 0.0008752947906032205\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight: 0.0003644223907031119\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias: 0.0003421250730752945\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight: 0.0007060670177452266\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight: 0.0008084061555564404\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight: 0.00037053594132885337\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias: 0.0003866026527248323\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight: 0.0008635701378807425\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias: 0.000405263650463894\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight: 0.0006636265316046774\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.1.norm1.bias: 0.00039357744390144944\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.1.norm2.bias: 0.00027385668363422155\n",
      "Weight diff for down_blocks.1.attentions.0.transformer_blocks.1.norm3.bias: 0.0003707110299728811\n",
      "Weight diff for down_blocks.1.attentions.1.norm.bias: 0.0004881092463620007\n",
      "Weight diff for down_blocks.1.attentions.1.proj_in.bias: 0.0004715912218671292\n",
      "Weight diff for down_blocks.1.attentions.1.proj_in.weight: 0.0009173937723971903\n",
      "Weight diff for down_blocks.1.attentions.1.proj_out.bias: 0.00042532815132290125\n",
      "Weight diff for down_blocks.1.attentions.1.proj_out.weight: 0.0007197747472673655\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight: 0.0009237800841219723\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias: 0.0005011277971789241\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight: 0.000509412435349077\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight: 0.0007219243561848998\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight: 0.00034355255775153637\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight: 0.0006306635914370418\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias: 0.0005093389190733433\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight: 0.0005583675228990614\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight: 0.0008806497207842767\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight: 0.0004725464968942106\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias: 0.00034086586674675345\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight: 0.0007382906624116004\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias: 0.0005349274724721909\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight: 0.0008900051470845938\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias: 0.000357217388227582\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias: 0.00033598311711102724\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias: 0.0004901789361611009\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight: 0.00048729567788541317\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias: 0.00046130496775731444\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight: 0.0007748748175799847\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight: 0.0007331728702411056\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight: 0.0005946841556578875\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight: 0.0006088966620154679\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias: 0.00046590931015089154\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight: 0.0006106213550083339\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight: 0.0007466925890184939\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight: 0.0007111162994988263\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias: 0.00040350525523535907\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight: 0.0006340071558952332\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias: 0.0004792186082340777\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight: 0.000917890458367765\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.1.norm1.bias: 0.0005044260178692639\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.1.norm2.bias: 0.0006713782786391675\n",
      "Weight diff for down_blocks.1.attentions.1.transformer_blocks.1.norm3.bias: 0.00025884760543704033\n",
      "Weight diff for down_blocks.2.attentions.0.norm.bias: 0.00044841287308372557\n",
      "Weight diff for down_blocks.2.attentions.0.proj_in.bias: 0.0006196335889399052\n",
      "Weight diff for down_blocks.2.attentions.0.proj_in.weight: 0.0006109405076131225\n",
      "Weight diff for down_blocks.2.attentions.0.proj_out.bias: 0.0006763053243048489\n",
      "Weight diff for down_blocks.2.attentions.0.proj_out.weight: 0.0009597939206287265\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight: 0.0010574801126495004\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias: 0.0006950516253709793\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight: 0.0006224035751074553\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight: 0.0008838158682920039\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight: 0.0005166460759937763\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight: 0.0007538179634138942\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias: 0.0006899767904542387\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight: 0.0002568863856140524\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight: 0.0006909306393936276\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight: 0.000990346074104309\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias: 0.0007532674353569746\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight: 0.0006235744804143906\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias: 0.0004735772090498358\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight: 0.0009076233254745603\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias: 0.0005230722017586231\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias: 0.0019704943988472223\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias: 0.0008854318875819445\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_k.weight: 0.0006465372862294316\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.bias: 0.0006857942789793015\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.weight: 0.0006936224526725709\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_q.weight: 0.0008233975386247039\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_v.weight: 0.001021403819322586\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_k.weight: 0.0009694814798422158\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.bias: 0.0006551099941134453\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.weight: 0.0008097144891507924\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_q.weight: 0.0010240358533337712\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_v.weight: 0.0007686428143642843\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.bias: 0.00039957696571946144\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.weight: 0.0006999285542406142\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.bias: 0.00034438559669069946\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.weight: 0.0007098205387592316\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.1.norm1.bias: 0.000542996684089303\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.1.norm2.bias: 0.0019304354209452868\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.1.norm3.bias: 0.001178997801616788\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_k.weight: 0.00119438034016639\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.bias: 0.00039614542038179934\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.weight: 0.0007734032697044313\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_q.weight: 0.0009017491829581559\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_v.weight: 0.0008175204275175929\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_k.weight: 0.0007750930963084102\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.bias: 0.00038368673995137215\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.weight: 0.0005517779500223696\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_q.weight: 0.0009687424753792584\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_v.weight: 0.0010734631214290857\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.bias: 0.0007298695272766054\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.weight: 0.0008465546416118741\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.bias: 0.000579505693167448\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.weight: 0.0005482533015310764\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.2.norm1.bias: 0.0007184176938608289\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.2.norm2.bias: 0.0013566536363214254\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.2.norm3.bias: 0.0010852953419089317\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_k.weight: 0.0009917101124301553\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.bias: 0.0004967725835740566\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.weight: 0.0003846316831186414\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_q.weight: 0.0008963203290477395\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_v.weight: 0.0006347228772938251\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_k.weight: 0.00043925215140916407\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.bias: 0.00046489661326631904\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.weight: 0.00022866470681037754\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_q.weight: 0.0010958649218082428\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_v.weight: 0.0005841983947902918\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.bias: 0.00033897929824888706\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.weight: 0.00077389384387061\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.bias: 0.0003237194905523211\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.weight: 0.000553309335373342\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.3.norm1.bias: 0.00040554613224230707\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.3.norm2.bias: 0.0011361859505996108\n",
      "Weight diff for down_blocks.2.attentions.0.transformer_blocks.3.norm3.bias: 0.0006156578892841935\n",
      "Weight diff for down_blocks.2.attentions.1.norm.bias: 0.0007338541909120977\n",
      "Weight diff for down_blocks.2.attentions.1.proj_in.bias: 0.0005717604653909802\n",
      "Weight diff for down_blocks.2.attentions.1.proj_in.weight: 0.000587675953283906\n",
      "Weight diff for down_blocks.2.attentions.1.proj_out.bias: 0.0006630630232393742\n",
      "Weight diff for down_blocks.2.attentions.1.proj_out.weight: 0.0007071485742926598\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight: 0.000617471756413579\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias: 0.0005589531501755118\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight: 0.0005940964911133051\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight: 0.0007912876317277551\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight: 0.0005295249284245074\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight: 0.0010611851466819644\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias: 0.0005503626307472587\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight: 0.0007518685306422412\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight: 0.0005808065179735422\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight: 0.0010848039528355002\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias: 0.0009787662420421839\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight: 0.0009211815195158124\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias: 0.0004520221264101565\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight: 0.0005372390151023865\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias: 0.0006738794036209583\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias: 0.0016029152320697904\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias: 0.0005698676686733961\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_k.weight: 0.0007406655931845307\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.bias: 0.00036357849603518844\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.weight: 0.000650392088573426\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_q.weight: 0.0008137060212902725\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_v.weight: 0.0007956487243063748\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_k.weight: 0.0004290909564588219\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.bias: 0.0002926570305135101\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.weight: 0.0006981540354900062\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_q.weight: 0.0010022735223174095\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_v.weight: 0.001144167734310031\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.bias: 0.0004946195986121893\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.weight: 0.0004528920690063387\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.bias: 0.00031459867022931576\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.weight: 0.0009434488019905984\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.1.norm1.bias: 0.0004764352925121784\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.1.norm2.bias: 0.0017148428596556187\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.1.norm3.bias: 0.0006596308085136116\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_k.weight: 0.0007705949246883392\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.bias: 0.0002562239533290267\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.weight: 0.0004271998768672347\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_q.weight: 0.0008408530848100781\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_v.weight: 0.0006591122364625335\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_k.weight: 0.0009132787818089128\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.bias: 0.0003466001362539828\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.weight: 0.0006942557520233095\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_q.weight: 0.0009629938867874444\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_v.weight: 0.0005660514580085874\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.bias: 0.00044926523696631193\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.weight: 0.0008900340762920678\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.bias: 0.0003051893727388233\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.weight: 0.0004309361393097788\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.2.norm1.bias: 0.0003131905978079885\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.2.norm2.bias: 0.0013068786356598139\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.2.norm3.bias: 0.00031782500445842743\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_k.weight: 0.0005443328991532326\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.bias: 0.00040828497731126845\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.weight: 0.0004916238249279559\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_q.weight: 0.0006397608667612076\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_v.weight: 0.000604335917159915\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_k.weight: 0.0012057103449478745\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.bias: 0.00044552102917805314\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.weight: 0.0008661896572448313\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_q.weight: 0.0007383838528767228\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_v.weight: 0.000891474075615406\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.bias: 0.000439463765360415\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.weight: 0.000867534545250237\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.bias: 0.0007132316241040826\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.weight: 0.0011559564154595137\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.3.norm1.bias: 0.0007306450279429555\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.3.norm2.bias: 0.0010093944147229195\n",
      "Weight diff for down_blocks.2.attentions.1.transformer_blocks.3.norm3.bias: 0.00045342498924583197\n",
      "Weight diff for up_blocks.0.attentions.0.norm.bias: 0.0005510491319000721\n",
      "Weight diff for up_blocks.0.attentions.0.proj_in.bias: 0.0004672318755183369\n",
      "Weight diff for up_blocks.0.attentions.0.proj_in.weight: 0.0006212787120603025\n",
      "Weight diff for up_blocks.0.attentions.0.proj_out.bias: 0.0004672491631936282\n",
      "Weight diff for up_blocks.0.attentions.0.proj_out.weight: 0.0005623694742098451\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight: 0.0009215206955559552\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias: 0.0005205185152590275\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight: 0.000628312467597425\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight: 0.0008383463136851788\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight: 0.0007729102508164942\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight: 0.0006994119612500072\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias: 0.000527688767760992\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight: 0.0007866863161325455\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight: 0.0006176464376039803\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight: 0.0009456806001253426\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias: 0.0003867656923830509\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight: 0.000612870731856674\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias: 0.0005117821856401861\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight: 0.00048707518726587296\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.0.norm1.bias: 0.0005014999769628048\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.0.norm2.bias: 0.0010802014730870724\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.0.norm3.bias: 0.00031724240398034453\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_k.weight: 0.0008369267452508211\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.bias: 0.0004872146819252521\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.weight: 0.0005494245560839772\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_q.weight: 0.0008854141342453659\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_v.weight: 0.0005279866745695472\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_k.weight: 0.0007211558404378593\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.bias: 0.000457375543192029\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.weight: 0.0009368947939947248\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_q.weight: 0.0005314074223861098\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_v.weight: 0.0007810048991814256\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.bias: 0.0003712929901666939\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.weight: 0.0009273014729842544\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.bias: 0.0005582736921496689\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.weight: 0.0007714525563642383\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.1.norm1.bias: 0.00036722130607813597\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.1.norm2.bias: 0.0012750002788379788\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.1.norm3.bias: 0.0004837888991460204\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_k.weight: 0.0009232761804014444\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.bias: 0.0004676684911828488\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.weight: 0.0004880137857981026\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_q.weight: 0.0011338788317516446\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_v.weight: 0.0006126560037955642\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_k.weight: 0.0006930989911779761\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.bias: 0.000476657907711342\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.weight: 0.0013860578183084726\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_q.weight: 0.0007011588895693421\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_v.weight: 0.0009763342095538974\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.bias: 0.0006257769418880343\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.weight: 0.0006746054859831929\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.bias: 0.0005415664054453373\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.weight: 0.0007568742730654776\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.2.norm1.bias: 0.0004905673558823764\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.2.norm2.bias: 0.0019410743843764067\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.2.norm3.bias: 0.0005393906030803919\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_k.weight: 0.0009787546005100012\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.bias: 0.00037383224116638303\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.weight: 0.0007912000291980803\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_q.weight: 0.0005423788679763675\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_v.weight: 0.0008210614323616028\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_k.weight: 0.0010679198894649744\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.bias: 0.00047673314111307263\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.weight: 0.0005069742328487337\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_q.weight: 0.0006527742953039706\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_v.weight: 0.0008828957797959447\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.bias: 0.0003379496047273278\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.weight: 0.0008464044658467174\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.bias: 0.00037827366031706333\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.weight: 0.0005315823946148157\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.3.norm1.bias: 0.00044273369712755084\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.3.norm2.bias: 0.001566197955980897\n",
      "Weight diff for up_blocks.0.attentions.0.transformer_blocks.3.norm3.bias: 0.0004328728246036917\n",
      "Weight diff for up_blocks.0.attentions.1.norm.bias: 0.00042641983600333333\n",
      "Weight diff for up_blocks.0.attentions.1.proj_in.bias: 0.0003730809548869729\n",
      "Weight diff for up_blocks.0.attentions.1.proj_in.weight: 0.0007445373921655118\n",
      "Weight diff for up_blocks.0.attentions.1.proj_out.bias: 0.000688634580001235\n",
      "Weight diff for up_blocks.0.attentions.1.proj_out.weight: 0.0011154762469232082\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight: 0.00032277341233566403\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias: 0.00037978182081133127\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight: 0.0004566483257804066\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight: 0.0007635009242221713\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight: 0.00037852657260373235\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight: 0.0009507880313321948\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias: 0.00034270051401108503\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight: 0.0006349214818328619\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight: 0.0006319846725091338\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight: 0.0006074630073271692\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias: 0.0006048785871826112\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight: 0.0005442906985990703\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias: 0.0003682943060994148\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight: 0.0006427784683182836\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.0.norm1.bias: 0.0003414530074223876\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.0.norm2.bias: 0.0015629308763891459\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.0.norm3.bias: 0.0005758664337918162\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_k.weight: 0.000878406164702028\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.bias: 0.00023372290888801217\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.weight: 0.0006435569375753403\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_q.weight: 0.0006558147724717855\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_v.weight: 0.0007490874268114567\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_k.weight: 0.0013477872125804424\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.bias: 0.000306052272208035\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.weight: 0.0005939822876825929\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_q.weight: 0.0008295426378026605\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_v.weight: 0.0006866829935461283\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.bias: 0.0006597712053917348\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.weight: 0.0007211650954559445\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.bias: 0.0004105026600882411\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.weight: 0.0005325534730218351\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.1.norm1.bias: 0.0006102137267589569\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.1.norm2.bias: 0.002727298531681299\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.1.norm3.bias: 0.00025312526850029826\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_k.weight: 0.0011615297989919782\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.bias: 0.000397037947550416\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.weight: 0.0007958635687828064\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_q.weight: 0.0009068037616088986\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_v.weight: 0.0007168491720221937\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_k.weight: 0.001068679615855217\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.bias: 0.0005131799844093621\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.weight: 0.0008938921964727342\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_q.weight: 0.0011327876709401608\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_v.weight: 0.0006118891760706902\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.bias: 0.0006323885754682124\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.weight: 0.0005908659659326077\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.bias: 0.00036408076994121075\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.weight: 0.0002557040425017476\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.2.norm1.bias: 0.00041080894879996777\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.2.norm2.bias: 0.002170024672523141\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.2.norm3.bias: 0.0006497430149465799\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_k.weight: 0.0011443189578130841\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.bias: 0.000431625172495842\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.weight: 0.0004498407943174243\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_q.weight: 0.0005277481395751238\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_v.weight: 0.0007476266473531723\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_k.weight: 0.0004504079115577042\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.bias: 0.0004444704682100564\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.weight: 0.0007610652828589082\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_q.weight: 0.00042124761966988444\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_v.weight: 0.0006615801830776036\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.bias: 0.0005526352906599641\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.weight: 0.0006555395084433258\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.bias: 0.0003208615235053003\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.weight: 0.000492946186568588\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.3.norm1.bias: 0.00021447223844006658\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.3.norm2.bias: 0.0008349886629730463\n",
      "Weight diff for up_blocks.0.attentions.1.transformer_blocks.3.norm3.bias: 0.0005126503529027104\n",
      "Weight diff for up_blocks.0.attentions.2.norm.bias: 0.0002607842325232923\n",
      "Weight diff for up_blocks.0.attentions.2.proj_in.bias: 0.00029096496291458607\n",
      "Weight diff for up_blocks.0.attentions.2.proj_in.weight: 0.0006772525375708938\n",
      "Weight diff for up_blocks.0.attentions.2.proj_out.bias: 0.00046034337719902396\n",
      "Weight diff for up_blocks.0.attentions.2.proj_out.weight: 0.0009385170415043831\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight: 0.00071895238943398\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias: 0.0002885979483835399\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight: 0.0007657433161512017\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight: 0.0009728289442136884\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight: 0.000533239683136344\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight: 0.0010294016683474183\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias: 0.00028712250059470534\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight: 0.0011467165313661098\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight: 0.001309836283326149\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight: 0.0005756650352850556\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.bias: 0.0004117642529308796\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.weight: 0.0010012935381382704\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.bias: 0.00038981216493994\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.weight: 0.000851660268381238\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.0.norm1.bias: 0.0003965525538660586\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.0.norm2.bias: 0.0011418787762522697\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.0.norm3.bias: 0.0003501744940876961\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_k.weight: 0.0008059339597821236\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.bias: 0.0004152518231421709\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.weight: 0.000740599527489394\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_q.weight: 0.0008262554183602333\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_v.weight: 0.000480296672321856\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_k.weight: 0.0006577841122634709\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.bias: 0.00041712625534273684\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.weight: 0.0004667181638069451\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_q.weight: 0.000815371226053685\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_v.weight: 0.000710462627466768\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.bias: 0.0004981457605026662\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.weight: 0.0008380748913623393\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.bias: 0.00036589137744158506\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.weight: 0.0008651185780763626\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.1.norm1.bias: 0.0005000800592824817\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.1.norm2.bias: 0.0008415426127612591\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.1.norm3.bias: 0.00047095847548916936\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_k.weight: 0.0014486206928268075\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.bias: 0.0003372484934516251\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.weight: 0.0006581011693924665\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_q.weight: 0.0009464800241403282\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_v.weight: 0.0006878456333652139\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_k.weight: 0.000699615862686187\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.bias: 0.0003259712248109281\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.weight: 0.0007365787751041353\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_q.weight: 0.0005112253129482269\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_v.weight: 0.0005017109215259552\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.bias: 0.0006853481754660606\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.weight: 0.0009068998042494059\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.bias: 0.0005444327834993601\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.weight: 0.00074360566213727\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.2.norm1.bias: 0.0006475037662312388\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.2.norm2.bias: 0.0012752700131386518\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.2.norm3.bias: 0.0006599879707209766\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_k.weight: 0.0008428501314483583\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.bias: 0.0005459592211991549\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.weight: 0.0006063546170480549\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_q.weight: 0.0008258277084678411\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_v.weight: 0.0006045594345778227\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_k.weight: 0.0006783355493098497\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.bias: 0.0005413744365796447\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.weight: 0.0008826137636788189\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_q.weight: 0.0007895587477833033\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_v.weight: 0.000760976574383676\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.bias: 0.000543230096809566\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.weight: 0.0006245673866942525\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.bias: 0.0004692089860327542\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.weight: 0.0008686365326866508\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.3.norm1.bias: 0.0005324774538166821\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.3.norm2.bias: 0.0009804072324186563\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.3.norm3.bias: 0.0008871174068190157\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_k.weight: 0.0012669982388615608\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.bias: 0.0003558668540790677\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.weight: 0.0007829722599126399\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_q.weight: 0.001015734625980258\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_v.weight: 0.0008954494260251522\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_k.weight: 0.0007814111304469407\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.bias: 0.0003590043052099645\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.weight: 0.0007319356664083898\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_q.weight: 0.0005403267568908632\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_v.weight: 0.0007475462625734508\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.bias: 0.0007394027197733521\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.weight: 0.000669021625071764\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.bias: 0.0003544959472492337\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.weight: 0.0013022508937865496\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.4.norm1.bias: 0.0004943811800330877\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.4.norm2.bias: 0.0007400641916319728\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.4.norm3.bias: 0.0006040344014763832\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_k.weight: 0.0006663791136816144\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.bias: 0.00043911635293625295\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.weight: 0.0004902363289147615\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_q.weight: 0.00061604636721313\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_v.weight: 0.0008667797083035111\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_k.weight: 0.0005055859219282866\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.bias: 0.00044064014218747616\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.weight: 0.0007078857161104679\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_q.weight: 0.0006754299392923713\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_v.weight: 0.0007034516311250627\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.bias: 0.001000697840936482\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.weight: 0.0007635361980646849\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.bias: 0.00039133906830102205\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.weight: 0.0008673819829709828\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.5.norm1.bias: 0.0006149699329398572\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.5.norm2.bias: 0.0009892287198454142\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.5.norm3.bias: 0.0007716898107901216\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_k.weight: 0.0009137017768807709\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.bias: 0.0005141922156326473\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.weight: 0.0005945453885942698\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_q.weight: 0.0006510192761197686\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_v.weight: 0.0010232779895886779\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_k.weight: 0.0006850697682239115\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.bias: 0.0005159492138773203\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.weight: 0.0003190571442246437\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_q.weight: 0.0008467885199934244\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_v.weight: 0.000663231301587075\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.bias: 0.0007014533039182425\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.weight: 0.0009736248757690191\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.bias: 0.0005579853896051645\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.weight: 0.0009028265485540032\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.6.norm1.bias: 0.000983923440799117\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.6.norm2.bias: 0.0008554635569453239\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.6.norm3.bias: 0.00044464675011113286\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_k.weight: 0.0008606024202890694\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.bias: 0.0003114102000836283\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.weight: 0.0008220893214456737\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_q.weight: 0.0006214064196683466\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_v.weight: 0.0006824985612183809\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_k.weight: 0.00041728944052010775\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.bias: 0.0003091034013777971\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.weight: 0.0004977802745997906\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_q.weight: 0.0005398219218477607\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_v.weight: 0.0004613636410795152\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.bias: 0.0004994542105123401\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.weight: 0.0007156375213526189\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.bias: 0.00035053701139986515\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.weight: 0.0006711120950058103\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.7.norm1.bias: 0.002736083697527647\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.7.norm2.bias: 0.0013619859237223864\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.7.norm3.bias: 0.000591136165894568\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_k.weight: 0.0005107792094349861\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.bias: 0.00032756864675320685\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.weight: 0.0005442050751298666\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_q.weight: 0.0005223220796324313\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_v.weight: 0.0005787221598438919\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_k.weight: 0.0005021541146561503\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.bias: 0.0003258024516981095\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.weight: 0.0006676503689959645\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_q.weight: 0.0005809851572848856\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_v.weight: 0.0006515256827697158\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.bias: 0.00042298994958400726\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.weight: 0.0006934059201739728\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.bias: 0.00035036075860261917\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.weight: 0.0006801061099395156\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.8.norm1.bias: 0.0008844593539834023\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.8.norm2.bias: 0.0014582903822883964\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.8.norm3.bias: 0.0004288840282242745\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_k.weight: 0.0011407858692109585\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.bias: 0.0003418835694901645\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.weight: 0.000618184800259769\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_q.weight: 0.0008836130145937204\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_v.weight: 0.00048308641999028623\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_k.weight: 0.0006608470575883985\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.bias: 0.00033858130336739123\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.weight: 0.0008532031788490713\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_q.weight: 0.000642173457890749\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_v.weight: 0.0007648118189536035\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.bias: 0.0007327099447138608\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.weight: 0.0004593131016008556\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.bias: 0.00015746965073049068\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.weight: 0.0006522842450067401\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.9.norm1.bias: 0.0004049652488902211\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.9.norm2.bias: 0.0008217561990022659\n",
      "Weight diff for up_blocks.0.attentions.2.transformer_blocks.9.norm3.bias: 0.0006862460868433118\n",
      "Weight diff for up_blocks.1.attentions.0.norm.bias: 0.0008032482583075762\n",
      "Weight diff for up_blocks.1.attentions.0.proj_in.bias: 0.0006926958449184895\n",
      "Weight diff for up_blocks.1.attentions.0.proj_in.weight: 0.000714112538844347\n",
      "Weight diff for up_blocks.1.attentions.0.proj_out.bias: 0.00043061631731688976\n",
      "Weight diff for up_blocks.1.attentions.0.proj_out.weight: 0.0005278480239212513\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight: 0.0007002971251495183\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias: 0.0005841287784278393\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight: 0.0005864872946403921\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight: 0.0005849093431606889\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight: 0.000408099964261055\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight: 0.0008964258595369756\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias: 0.0005893122870475054\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight: 0.0006494172848761082\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight: 0.0009764604037627578\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight: 0.0006612938595935702\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias: 0.00027665332891047\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight: 0.001178594189696014\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias: 0.0006294736522249877\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight: 0.000648082117550075\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias: 0.0006134501891210675\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias: 0.00046089570969343185\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias: 0.0004929519491270185\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight: 0.0006215302273631096\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias: 0.0005932741914875805\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight: 0.0007590040331706405\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight: 0.0008543066214770079\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight: 0.0005998907145112753\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight: 0.0005523678264580667\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias: 0.0005817041965201497\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight: 0.0007878298638388515\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight: 0.0005507215391844511\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight: 0.0003102263726759702\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias: 0.0004276447289157659\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight: 0.000689798966050148\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias: 0.000458846683613956\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight: 0.0006011618534103036\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.1.norm1.bias: 0.0003858339332509786\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.1.norm2.bias: 0.0005655988352373242\n",
      "Weight diff for up_blocks.1.attentions.0.transformer_blocks.1.norm3.bias: 0.0006896198028698564\n",
      "Weight diff for up_blocks.1.attentions.1.norm.bias: 0.0005280193872749805\n",
      "Weight diff for up_blocks.1.attentions.1.proj_in.bias: 0.00023713777773082256\n",
      "Weight diff for up_blocks.1.attentions.1.proj_in.weight: 0.0009072177927009761\n",
      "Weight diff for up_blocks.1.attentions.1.proj_out.bias: 0.0003552221751306206\n",
      "Weight diff for up_blocks.1.attentions.1.proj_out.weight: 0.0005206072237342596\n",
      "Weight diff for up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight: 0.0006209511775523424\n",
      "Weight diff for up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias: 0.00023135602532420307\n",
      "Weight diff for up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight: 0.000636702636256814\n",
      "Weight diff for up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight: 0.00025895488215610385\n",
      "Weight diff for up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight: 0.0001769661612343043\n",
      "Weight diff for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight: 0.00042366189882159233\n",
      "Weight diff for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias: 0.00031797902192920446\n",
      "Weight diff for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight: 0.0010155364871025085\n",
      "Weight diff for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight: 0.000655363081023097\n",
      "Weight diff for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight: 0.0008518231334164739\n",
      "Weight diff for up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias: 0.0005376094486564398\n",
      "Weight diff for up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight: 0.0010973942698910832\n",
      "Weight diff for up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias: 0.0003529977984726429\n",
      "Weight diff for up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight: 0.0006556652369908988\n",
      "Weight diff for up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias: 0.0004215413937345147\n",
      "Weight diff for up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias: 0.0009140255860984325\n",
      "Weight diff for up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias: 0.000299201812595129\n",
      "Weight diff for up_blocks.1.attentions.2.norm.bias: 0.000161830335855484\n",
      "Weight diff for up_blocks.1.attentions.2.proj_in.bias: 0.0003767628804780543\n",
      "Weight diff for up_blocks.1.attentions.2.proj_in.weight: 0.0005536791868507862\n",
      "Weight diff for up_blocks.1.attentions.2.proj_out.bias: 0.0005800767103210092\n",
      "Weight diff for up_blocks.1.attentions.2.proj_out.weight: 0.001043465337716043\n",
      "Weight diff for up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight: 0.000676075229421258\n",
      "Weight diff for up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias: 0.00018673253362067044\n",
      "Weight diff for up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight: 0.0003126728115603328\n",
      "Weight diff for up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight: 0.0018087513744831085\n",
      "Weight diff for up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight: 0.00035156786907464266\n",
      "Weight diff for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight: 0.0002449717139825225\n",
      "Weight diff for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias: 0.00022270693443715572\n",
      "Weight diff for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight: 0.0006277295760810375\n",
      "Weight diff for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight: 0.0006170137203298509\n",
      "Weight diff for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight: 0.0009355306974612176\n",
      "Weight diff for up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias: 0.0004996218485757709\n",
      "Weight diff for up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight: 0.0008018956868909299\n",
      "Weight diff for up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias: 0.00043961836490780115\n",
      "Weight diff for up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight: 0.0005386549746617675\n",
      "Weight diff for up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias: 0.00042354315519332886\n",
      "Weight diff for up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias: 0.00034524500370025635\n",
      "Weight diff for up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias: 0.00029470864683389664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['conv_in.weight', 'conv_in.bias', 'time_embedding.linear_1.weight', 'time_embedding.linear_1.bias', 'time_embedding.linear_2.weight', 'time_embedding.linear_2.bias', 'add_embedding.linear_1.weight', 'add_embedding.linear_1.bias', 'add_embedding.linear_2.weight', 'add_embedding.linear_2.bias', 'down_blocks.0.resnets.0.norm1.weight', 'down_blocks.0.resnets.0.norm1.bias', 'down_blocks.0.resnets.0.conv1.weight', 'down_blocks.0.resnets.0.conv1.bias', 'down_blocks.0.resnets.0.time_emb_proj.weight', 'down_blocks.0.resnets.0.time_emb_proj.bias', 'down_blocks.0.resnets.0.norm2.weight', 'down_blocks.0.resnets.0.norm2.bias', 'down_blocks.0.resnets.0.conv2.weight', 'down_blocks.0.resnets.0.conv2.bias', 'down_blocks.0.resnets.1.norm1.weight', 'down_blocks.0.resnets.1.norm1.bias', 'down_blocks.0.resnets.1.conv1.weight', 'down_blocks.0.resnets.1.conv1.bias', 'down_blocks.0.resnets.1.time_emb_proj.weight', 'down_blocks.0.resnets.1.time_emb_proj.bias', 'down_blocks.0.resnets.1.norm2.weight', 'down_blocks.0.resnets.1.norm2.bias', 'down_blocks.0.resnets.1.conv2.weight', 'down_blocks.0.resnets.1.conv2.bias', 'down_blocks.0.downsamplers.0.conv.weight', 'down_blocks.0.downsamplers.0.conv.bias', 'down_blocks.1.attentions.0.norm.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'down_blocks.1.attentions.0.transformer_blocks.1.norm1.weight', 'down_blocks.1.attentions.0.transformer_blocks.1.norm2.weight', 'down_blocks.1.attentions.0.transformer_blocks.1.norm3.weight', 'down_blocks.1.attentions.1.norm.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'down_blocks.1.attentions.1.transformer_blocks.1.norm1.weight', 'down_blocks.1.attentions.1.transformer_blocks.1.norm2.weight', 'down_blocks.1.attentions.1.transformer_blocks.1.norm3.weight', 'down_blocks.1.resnets.0.norm1.weight', 'down_blocks.1.resnets.0.norm1.bias', 'down_blocks.1.resnets.0.conv1.weight', 'down_blocks.1.resnets.0.conv1.bias', 'down_blocks.1.resnets.0.time_emb_proj.weight', 'down_blocks.1.resnets.0.time_emb_proj.bias', 'down_blocks.1.resnets.0.norm2.weight', 'down_blocks.1.resnets.0.norm2.bias', 'down_blocks.1.resnets.0.conv2.weight', 'down_blocks.1.resnets.0.conv2.bias', 'down_blocks.1.resnets.0.conv_shortcut.weight', 'down_blocks.1.resnets.0.conv_shortcut.bias', 'down_blocks.1.resnets.1.norm1.weight', 'down_blocks.1.resnets.1.norm1.bias', 'down_blocks.1.resnets.1.conv1.weight', 'down_blocks.1.resnets.1.conv1.bias', 'down_blocks.1.resnets.1.time_emb_proj.weight', 'down_blocks.1.resnets.1.time_emb_proj.bias', 'down_blocks.1.resnets.1.norm2.weight', 'down_blocks.1.resnets.1.norm2.bias', 'down_blocks.1.resnets.1.conv2.weight', 'down_blocks.1.resnets.1.conv2.bias', 'down_blocks.1.downsamplers.0.conv.weight', 'down_blocks.1.downsamplers.0.conv.bias', 'down_blocks.2.attentions.0.norm.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'down_blocks.2.attentions.0.transformer_blocks.1.norm1.weight', 'down_blocks.2.attentions.0.transformer_blocks.1.norm2.weight', 'down_blocks.2.attentions.0.transformer_blocks.1.norm3.weight', 'down_blocks.2.attentions.0.transformer_blocks.2.norm1.weight', 'down_blocks.2.attentions.0.transformer_blocks.2.norm2.weight', 'down_blocks.2.attentions.0.transformer_blocks.2.norm3.weight', 'down_blocks.2.attentions.0.transformer_blocks.3.norm1.weight', 'down_blocks.2.attentions.0.transformer_blocks.3.norm2.weight', 'down_blocks.2.attentions.0.transformer_blocks.3.norm3.weight', 'down_blocks.2.attentions.1.norm.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'down_blocks.2.attentions.1.transformer_blocks.1.norm1.weight', 'down_blocks.2.attentions.1.transformer_blocks.1.norm2.weight', 'down_blocks.2.attentions.1.transformer_blocks.1.norm3.weight', 'down_blocks.2.attentions.1.transformer_blocks.2.norm1.weight', 'down_blocks.2.attentions.1.transformer_blocks.2.norm2.weight', 'down_blocks.2.attentions.1.transformer_blocks.2.norm3.weight', 'down_blocks.2.attentions.1.transformer_blocks.3.norm1.weight', 'down_blocks.2.attentions.1.transformer_blocks.3.norm2.weight', 'down_blocks.2.attentions.1.transformer_blocks.3.norm3.weight', 'down_blocks.2.resnets.0.norm1.weight', 'down_blocks.2.resnets.0.norm1.bias', 'down_blocks.2.resnets.0.conv1.weight', 'down_blocks.2.resnets.0.conv1.bias', 'down_blocks.2.resnets.0.time_emb_proj.weight', 'down_blocks.2.resnets.0.time_emb_proj.bias', 'down_blocks.2.resnets.0.norm2.weight', 'down_blocks.2.resnets.0.norm2.bias', 'down_blocks.2.resnets.0.conv2.weight', 'down_blocks.2.resnets.0.conv2.bias', 'down_blocks.2.resnets.0.conv_shortcut.weight', 'down_blocks.2.resnets.0.conv_shortcut.bias', 'down_blocks.2.resnets.1.norm1.weight', 'down_blocks.2.resnets.1.norm1.bias', 'down_blocks.2.resnets.1.conv1.weight', 'down_blocks.2.resnets.1.conv1.bias', 'down_blocks.2.resnets.1.time_emb_proj.weight', 'down_blocks.2.resnets.1.time_emb_proj.bias', 'down_blocks.2.resnets.1.norm2.weight', 'down_blocks.2.resnets.1.norm2.bias', 'down_blocks.2.resnets.1.conv2.weight', 'down_blocks.2.resnets.1.conv2.bias', 'up_blocks.0.attentions.0.norm.weight', 'up_blocks.0.attentions.0.transformer_blocks.0.norm1.weight', 'up_blocks.0.attentions.0.transformer_blocks.0.norm2.weight', 'up_blocks.0.attentions.0.transformer_blocks.0.norm3.weight', 'up_blocks.0.attentions.0.transformer_blocks.1.norm1.weight', 'up_blocks.0.attentions.0.transformer_blocks.1.norm2.weight', 'up_blocks.0.attentions.0.transformer_blocks.1.norm3.weight', 'up_blocks.0.attentions.0.transformer_blocks.2.norm1.weight', 'up_blocks.0.attentions.0.transformer_blocks.2.norm2.weight', 'up_blocks.0.attentions.0.transformer_blocks.2.norm3.weight', 'up_blocks.0.attentions.0.transformer_blocks.3.norm1.weight', 'up_blocks.0.attentions.0.transformer_blocks.3.norm2.weight', 'up_blocks.0.attentions.0.transformer_blocks.3.norm3.weight', 'up_blocks.0.attentions.1.norm.weight', 'up_blocks.0.attentions.1.transformer_blocks.0.norm1.weight', 'up_blocks.0.attentions.1.transformer_blocks.0.norm2.weight', 'up_blocks.0.attentions.1.transformer_blocks.0.norm3.weight', 'up_blocks.0.attentions.1.transformer_blocks.1.norm1.weight', 'up_blocks.0.attentions.1.transformer_blocks.1.norm2.weight', 'up_blocks.0.attentions.1.transformer_blocks.1.norm3.weight', 'up_blocks.0.attentions.1.transformer_blocks.2.norm1.weight', 'up_blocks.0.attentions.1.transformer_blocks.2.norm2.weight', 'up_blocks.0.attentions.1.transformer_blocks.2.norm3.weight', 'up_blocks.0.attentions.1.transformer_blocks.3.norm1.weight', 'up_blocks.0.attentions.1.transformer_blocks.3.norm2.weight', 'up_blocks.0.attentions.1.transformer_blocks.3.norm3.weight', 'up_blocks.0.attentions.2.norm.weight', 'up_blocks.0.attentions.2.transformer_blocks.0.norm1.weight', 'up_blocks.0.attentions.2.transformer_blocks.0.norm2.weight', 'up_blocks.0.attentions.2.transformer_blocks.0.norm3.weight', 'up_blocks.0.attentions.2.transformer_blocks.1.norm1.weight', 'up_blocks.0.attentions.2.transformer_blocks.1.norm2.weight', 'up_blocks.0.attentions.2.transformer_blocks.1.norm3.weight', 'up_blocks.0.attentions.2.transformer_blocks.2.norm1.weight', 'up_blocks.0.attentions.2.transformer_blocks.2.norm2.weight', 'up_blocks.0.attentions.2.transformer_blocks.2.norm3.weight', 'up_blocks.0.attentions.2.transformer_blocks.3.norm1.weight', 'up_blocks.0.attentions.2.transformer_blocks.3.norm2.weight', 'up_blocks.0.attentions.2.transformer_blocks.3.norm3.weight', 'up_blocks.0.attentions.2.transformer_blocks.4.norm1.weight', 'up_blocks.0.attentions.2.transformer_blocks.4.norm2.weight', 'up_blocks.0.attentions.2.transformer_blocks.4.norm3.weight', 'up_blocks.0.attentions.2.transformer_blocks.5.norm1.weight', 'up_blocks.0.attentions.2.transformer_blocks.5.norm2.weight', 'up_blocks.0.attentions.2.transformer_blocks.5.norm3.weight', 'up_blocks.0.attentions.2.transformer_blocks.6.norm1.weight', 'up_blocks.0.attentions.2.transformer_blocks.6.norm2.weight', 'up_blocks.0.attentions.2.transformer_blocks.6.norm3.weight', 'up_blocks.0.attentions.2.transformer_blocks.7.norm1.weight', 'up_blocks.0.attentions.2.transformer_blocks.7.norm2.weight', 'up_blocks.0.attentions.2.transformer_blocks.7.norm3.weight', 'up_blocks.0.attentions.2.transformer_blocks.8.norm1.weight', 'up_blocks.0.attentions.2.transformer_blocks.8.norm2.weight', 'up_blocks.0.attentions.2.transformer_blocks.8.norm3.weight', 'up_blocks.0.attentions.2.transformer_blocks.9.norm1.weight', 'up_blocks.0.attentions.2.transformer_blocks.9.norm2.weight', 'up_blocks.0.attentions.2.transformer_blocks.9.norm3.weight', 'up_blocks.0.resnets.0.norm1.weight', 'up_blocks.0.resnets.0.norm1.bias', 'up_blocks.0.resnets.0.conv1.weight', 'up_blocks.0.resnets.0.conv1.bias', 'up_blocks.0.resnets.0.time_emb_proj.weight', 'up_blocks.0.resnets.0.time_emb_proj.bias', 'up_blocks.0.resnets.0.norm2.weight', 'up_blocks.0.resnets.0.norm2.bias', 'up_blocks.0.resnets.0.conv2.weight', 'up_blocks.0.resnets.0.conv2.bias', 'up_blocks.0.resnets.0.conv_shortcut.weight', 'up_blocks.0.resnets.0.conv_shortcut.bias', 'up_blocks.0.resnets.1.norm1.weight', 'up_blocks.0.resnets.1.norm1.bias', 'up_blocks.0.resnets.1.conv1.weight', 'up_blocks.0.resnets.1.conv1.bias', 'up_blocks.0.resnets.1.time_emb_proj.weight', 'up_blocks.0.resnets.1.time_emb_proj.bias', 'up_blocks.0.resnets.1.norm2.weight', 'up_blocks.0.resnets.1.norm2.bias', 'up_blocks.0.resnets.1.conv2.weight', 'up_blocks.0.resnets.1.conv2.bias', 'up_blocks.0.resnets.1.conv_shortcut.weight', 'up_blocks.0.resnets.1.conv_shortcut.bias', 'up_blocks.0.resnets.2.norm1.weight', 'up_blocks.0.resnets.2.norm1.bias', 'up_blocks.0.resnets.2.conv1.weight', 'up_blocks.0.resnets.2.conv1.bias', 'up_blocks.0.resnets.2.time_emb_proj.weight', 'up_blocks.0.resnets.2.time_emb_proj.bias', 'up_blocks.0.resnets.2.norm2.weight', 'up_blocks.0.resnets.2.norm2.bias', 'up_blocks.0.resnets.2.conv2.weight', 'up_blocks.0.resnets.2.conv2.bias', 'up_blocks.0.resnets.2.conv_shortcut.weight', 'up_blocks.0.resnets.2.conv_shortcut.bias', 'up_blocks.0.upsamplers.0.conv.weight', 'up_blocks.0.upsamplers.0.conv.bias', 'up_blocks.1.attentions.0.norm.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'up_blocks.1.attentions.0.transformer_blocks.1.norm1.weight', 'up_blocks.1.attentions.0.transformer_blocks.1.norm2.weight', 'up_blocks.1.attentions.0.transformer_blocks.1.norm3.weight', 'up_blocks.1.attentions.1.norm.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'up_blocks.1.attentions.2.norm.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight', 'up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight', 'up_blocks.1.resnets.0.norm1.weight', 'up_blocks.1.resnets.0.norm1.bias', 'up_blocks.1.resnets.0.conv1.weight', 'up_blocks.1.resnets.0.conv1.bias', 'up_blocks.1.resnets.0.time_emb_proj.weight', 'up_blocks.1.resnets.0.time_emb_proj.bias', 'up_blocks.1.resnets.0.norm2.weight', 'up_blocks.1.resnets.0.norm2.bias', 'up_blocks.1.resnets.0.conv2.weight', 'up_blocks.1.resnets.0.conv2.bias', 'up_blocks.1.resnets.0.conv_shortcut.weight', 'up_blocks.1.resnets.0.conv_shortcut.bias', 'up_blocks.1.resnets.1.norm1.weight', 'up_blocks.1.resnets.1.norm1.bias', 'up_blocks.1.resnets.1.conv1.weight', 'up_blocks.1.resnets.1.conv1.bias', 'up_blocks.1.resnets.1.time_emb_proj.weight', 'up_blocks.1.resnets.1.time_emb_proj.bias', 'up_blocks.1.resnets.1.norm2.weight', 'up_blocks.1.resnets.1.norm2.bias', 'up_blocks.1.resnets.1.conv2.weight', 'up_blocks.1.resnets.1.conv2.bias', 'up_blocks.1.resnets.1.conv_shortcut.weight', 'up_blocks.1.resnets.1.conv_shortcut.bias', 'up_blocks.1.resnets.2.norm1.weight', 'up_blocks.1.resnets.2.norm1.bias', 'up_blocks.1.resnets.2.conv1.weight', 'up_blocks.1.resnets.2.conv1.bias', 'up_blocks.1.resnets.2.time_emb_proj.weight', 'up_blocks.1.resnets.2.time_emb_proj.bias', 'up_blocks.1.resnets.2.norm2.weight', 'up_blocks.1.resnets.2.norm2.bias', 'up_blocks.1.resnets.2.conv2.weight', 'up_blocks.1.resnets.2.conv2.bias', 'up_blocks.1.resnets.2.conv_shortcut.weight', 'up_blocks.1.resnets.2.conv_shortcut.bias', 'up_blocks.1.upsamplers.0.conv.weight', 'up_blocks.1.upsamplers.0.conv.bias', 'up_blocks.2.resnets.0.norm1.weight', 'up_blocks.2.resnets.0.norm1.bias', 'up_blocks.2.resnets.0.conv1.weight', 'up_blocks.2.resnets.0.conv1.bias', 'up_blocks.2.resnets.0.time_emb_proj.weight', 'up_blocks.2.resnets.0.time_emb_proj.bias', 'up_blocks.2.resnets.0.norm2.weight', 'up_blocks.2.resnets.0.norm2.bias', 'up_blocks.2.resnets.0.conv2.weight', 'up_blocks.2.resnets.0.conv2.bias', 'up_blocks.2.resnets.0.conv_shortcut.weight', 'up_blocks.2.resnets.0.conv_shortcut.bias', 'up_blocks.2.resnets.1.norm1.weight', 'up_blocks.2.resnets.1.norm1.bias', 'up_blocks.2.resnets.1.conv1.weight', 'up_blocks.2.resnets.1.conv1.bias', 'up_blocks.2.resnets.1.time_emb_proj.weight', 'up_blocks.2.resnets.1.time_emb_proj.bias', 'up_blocks.2.resnets.1.norm2.weight', 'up_blocks.2.resnets.1.norm2.bias', 'up_blocks.2.resnets.1.conv2.weight', 'up_blocks.2.resnets.1.conv2.bias', 'up_blocks.2.resnets.1.conv_shortcut.weight', 'up_blocks.2.resnets.1.conv_shortcut.bias', 'up_blocks.2.resnets.2.norm1.weight', 'up_blocks.2.resnets.2.norm1.bias', 'up_blocks.2.resnets.2.conv1.weight', 'up_blocks.2.resnets.2.conv1.bias', 'up_blocks.2.resnets.2.time_emb_proj.weight', 'up_blocks.2.resnets.2.time_emb_proj.bias', 'up_blocks.2.resnets.2.norm2.weight', 'up_blocks.2.resnets.2.norm2.bias', 'up_blocks.2.resnets.2.conv2.weight', 'up_blocks.2.resnets.2.conv2.bias', 'up_blocks.2.resnets.2.conv_shortcut.weight', 'up_blocks.2.resnets.2.conv_shortcut.bias', 'mid_block.resnets.0.norm1.weight', 'mid_block.resnets.0.norm1.bias', 'mid_block.resnets.0.conv1.weight', 'mid_block.resnets.0.conv1.bias', 'mid_block.resnets.0.time_emb_proj.weight', 'mid_block.resnets.0.time_emb_proj.bias', 'mid_block.resnets.0.norm2.weight', 'mid_block.resnets.0.norm2.bias', 'mid_block.resnets.0.conv2.weight', 'mid_block.resnets.0.conv2.bias', 'conv_norm_out.weight', 'conv_norm_out.bias', 'conv_out.weight', 'conv_out.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"segmind/SSD-1B\",\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "tensors = load_file(\"/root/bigdisk/project_structured_prompt/stage1_sdxl_training/checkpoint_lr_high/unet/checkpoint-12001.unet.safetensors\")\n",
    "prefix = 'tt'\n",
    "\n",
    "with torch.no_grad():\n",
    "    model = pipe.unet\n",
    "    prefix = \"finetuned\"\n",
    "    for k, v in tensors.items():\n",
    "        print(\n",
    "            f\"Weight diff for {k}: {torch.abs(model.state_dict()[k].flatten()[:8].cpu() - v.flatten()[:8].cpu()).mean()}\"\n",
    "        )\n",
    "        v = v.to(\"cuda\").to(torch.float16)\n",
    "\n",
    "pipe.unet.load_state_dict(tensors, strict=False)  # should take < 2 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jason the killer movie cover', 'Leonard, hairy baby child actor Johnny Galecki, beautiful eyes, realistic toddler, portrait, black, dark, dark background', 'goddess, guest', 'Tyler Hoechlin, Slytherin robes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:15<00:00,  3.32it/s]\n"
     ]
    }
   ],
   "source": [
    "gen = torch.Generator().manual_seed(10)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw, ImageFont\n",
    "\n",
    "data = pd.read_csv(\"/root/bigdisk/project_structured_prompt/stage0_prompt_decomposition/scripts/journeydb_subsampled.csv\")\n",
    "# random split with seed 0\n",
    "data = data.sample(frac=1, random_state=0)\n",
    "\n",
    "data = data.iloc[int(len(data) * 0.8) :]\n",
    "\n",
    "# reset index\n",
    "data = data.reset_index(drop=True)\n",
    "STEP = 4\n",
    "for i in range(0, 2, STEP):\n",
    "    captions = data[\"content\"][i:i + STEP].tolist()\n",
    "    captions_2 = data[\"style\"][i:i + STEP].tolist()\n",
    "    original_images = data[\"img_path\"][i:i + STEP].tolist()\n",
    "    print(captions)\n",
    "    img = pipe(\n",
    "        height=1024,\n",
    "        width=1024,\n",
    "        prompt=captions,\n",
    "        prompt_2=captions_2,\n",
    "        generator=gen,\n",
    "        guidance_scale=4.0,\n",
    "    ).images\n",
    "    for j, im in enumerate(img):\n",
    "        org_im = Image.open(original_images[j])\n",
    "        # concat images\n",
    "        \n",
    "        org_im = org_im.resize((512, 512))\n",
    "        org_im = Image.fromarray(np.array(org_im))\n",
    "        org_im = np.array(org_im)\n",
    "\n",
    "        im = im.resize((512, 512))\n",
    "        im = np.array(im)\n",
    "        \n",
    "        im = np.concatenate((org_im, im), axis=1)\n",
    "        # put the caption on bottom of the iamge\n",
    "    \n",
    "        im = Image.fromarray(im)\n",
    "        \n",
    "        caption = captions[j] + \"\\n\" + captions_2[j]\n",
    "        font = ImageFont.load_default()\n",
    "        text_w, text_h = 25, 25\n",
    "        caption_image = Image.new('RGB', (im.width, im.height + text_h), (255, 255, 255))\n",
    "        draw = ImageDraw.Draw(caption_image)\n",
    "        caption_image.paste(im, (0, 0))\n",
    "        draw.text((0, im.height), caption, font=font, fill='black')\n",
    "\n",
    "        im = caption_image\n",
    "                \n",
    "            \n",
    "\n",
    "        im.save(f\"images/{i + j}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu122py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
